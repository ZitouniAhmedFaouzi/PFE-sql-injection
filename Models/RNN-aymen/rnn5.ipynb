{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa7a656-3258-4f64-9d36-7e8cd0b19b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "383/383 [==============================] - 41s 94ms/step - loss: 0.0574 - accuracy: 0.9840 - val_loss: 0.0176 - val_accuracy: 0.9964\n",
      "Epoch 2/20\n",
      "383/383 [==============================] - 45s 118ms/step - loss: 0.0145 - accuracy: 0.9966 - val_loss: 0.0220 - val_accuracy: 0.9918\n",
      "Epoch 3/20\n",
      "383/383 [==============================] - 34s 89ms/step - loss: 0.0101 - accuracy: 0.9982 - val_loss: 0.0153 - val_accuracy: 0.9967\n",
      "Epoch 4/20\n",
      "383/383 [==============================] - 34s 89ms/step - loss: 0.0096 - accuracy: 0.9981 - val_loss: 0.0180 - val_accuracy: 0.9943\n",
      "Epoch 5/20\n",
      "383/383 [==============================] - 34s 89ms/step - loss: 0.0077 - accuracy: 0.9989 - val_loss: 0.0138 - val_accuracy: 0.9967\n",
      "Epoch 6/20\n",
      "383/383 [==============================] - 34s 90ms/step - loss: 0.0067 - accuracy: 0.9989 - val_loss: 0.0182 - val_accuracy: 0.9959\n",
      "Epoch 7/20\n",
      "383/383 [==============================] - 34s 89ms/step - loss: 0.0068 - accuracy: 0.9990 - val_loss: 0.0216 - val_accuracy: 0.9959\n",
      "Epoch 8/20\n",
      "383/383 [==============================] - 35s 91ms/step - loss: 0.0073 - accuracy: 0.9988 - val_loss: 0.0147 - val_accuracy: 0.9967\n",
      "Epoch 9/20\n",
      "383/383 [==============================] - 38s 98ms/step - loss: 0.0075 - accuracy: 0.9986 - val_loss: 0.0238 - val_accuracy: 0.9961\n",
      "Epoch 10/20\n",
      "383/383 [==============================] - 35s 92ms/step - loss: 0.0075 - accuracy: 0.9988 - val_loss: 0.0258 - val_accuracy: 0.9954\n",
      "Epoch 11/20\n",
      "383/383 [==============================] - 35s 91ms/step - loss: 0.0066 - accuracy: 0.9990 - val_loss: 0.0209 - val_accuracy: 0.9954\n",
      "Epoch 12/20\n",
      "383/383 [==============================] - 35s 92ms/step - loss: 0.0065 - accuracy: 0.9990 - val_loss: 0.0268 - val_accuracy: 0.9951\n",
      "Epoch 13/20\n",
      "383/383 [==============================] - 35s 91ms/step - loss: 0.0064 - accuracy: 0.9990 - val_loss: 0.0184 - val_accuracy: 0.9962\n",
      "Epoch 14/20\n",
      "383/383 [==============================] - 35s 90ms/step - loss: 0.0065 - accuracy: 0.9990 - val_loss: 0.0263 - val_accuracy: 0.9953\n",
      "Epoch 15/20\n",
      "383/383 [==============================] - 35s 91ms/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 0.0212 - val_accuracy: 0.9961\n",
      "Epoch 16/20\n",
      "383/383 [==============================] - 35s 90ms/step - loss: 0.0064 - accuracy: 0.9990 - val_loss: 0.0238 - val_accuracy: 0.9953\n",
      "Epoch 17/20\n",
      "383/383 [==============================] - 35s 90ms/step - loss: 0.0107 - accuracy: 0.9977 - val_loss: 0.0188 - val_accuracy: 0.9959\n",
      "Epoch 18/20\n",
      "383/383 [==============================] - 35s 92ms/step - loss: 0.0076 - accuracy: 0.9988 - val_loss: 0.0180 - val_accuracy: 0.9966\n",
      "Epoch 19/20\n",
      "383/383 [==============================] - 35s 91ms/step - loss: 0.0066 - accuracy: 0.9990 - val_loss: 0.0242 - val_accuracy: 0.9959\n",
      "Epoch 20/20\n",
      "383/383 [==============================] - 35s 92ms/step - loss: 0.0089 - accuracy: 0.9980 - val_loss: 0.0281 - val_accuracy: 0.9958\n",
      "1054/1054 [==============================] - 19s 18ms/step - loss: 0.0394 - accuracy: 0.9933\n",
      "\n",
      "Test Accuracy: 0.9933\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Dropout, Embedding\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# 1. Chargement des donn√©es\n",
    "train_data = pd.read_csv('SQLIV3_cleaned2.csv')\n",
    "test_data = pd.read_csv('sqliv2_utf8.csv')\n",
    "\n",
    "# 2. Suppression des doublons (en gardant la premi√®re occurrence)\n",
    "train_data.drop_duplicates(subset='Sentence', keep='first', inplace=True)\n",
    "test_data.drop_duplicates(subset='Sentence', keep='first', inplace=True)\n",
    "\n",
    "# 3. Nettoyage MINIMAL (on conserve les caract√®res sp√©ciaux !)\n",
    "def clean_text(text):\n",
    "    text = str(text).strip()  # Conversion en string + suppression espaces inutiles\n",
    "    return text\n",
    "\n",
    "train_data['Sentence'] = train_data['Sentence'].apply(clean_text)\n",
    "test_data['Sentence'] = test_data['Sentence'].apply(clean_text)\n",
    "\n",
    "# 4. Tokenisation (on garde tous les caract√®res)\n",
    "vocab_size = 15000  # Vocabulaire large pour les motifs SQL\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=vocab_size,\n",
    "    oov_token=\"<OOV>\",\n",
    "    filters='',       # ‚Üê AUCUN filtre (conserve ', \", ;, -- etc.)\n",
    "    lower=False       # ‚Üê Conserve la casse (important pour SQL)\n",
    ")\n",
    "tokenizer.fit_on_texts(train_data['Sentence'])\n",
    "\n",
    "# 5. Padding adaptatif\n",
    "max_len = int(np.percentile([len(x.split()) for x in train_data['Sentence']], 95))\n",
    "X = tokenizer.texts_to_sequences(train_data['Sentence'])\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "y = train_data['Label'].astype('int')\n",
    "\n",
    "# 6. Split train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 7. Architecture du mod√®le\n",
    "embedding_dim = 256  # Grande dimension pour les caract√®res sp√©ciaux\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "    SimpleRNN(256, return_sequences=True),  # Couche 1: capture les motifs locaux\n",
    "    Dropout(0.3),\n",
    "    SimpleRNN(128),                         # Couche 2: agr√®ge les motifs\n",
    "    Dense(64, activation='relu'),           # Couche dense interm√©diaire\n",
    "    Dense(1, activation='sigmoid')          # Sortie binaire\n",
    "])\n",
    "\n",
    "# 8. Optimisation\n",
    "optimizer = optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# 9. Entra√Ænement\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    ")\n",
    "\n",
    "# 10. √âvaluation\n",
    "X_test = tokenizer.texts_to_sequences(test_data['Sentence'])\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=max_len)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, test_data['Label'].astype('int'))\n",
    "print(f'\\nTest Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d45ae6d-7d83-448f-93c6-8b9ce1f01065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Nombre total de requ√™tes spam dans test_data : 11424\n",
      "357/357 [==============================] - 6s 16ms/step\n",
      "‚úÖ Spams correctement d√©tect√©s : 11353\n",
      "üìä Taux de d√©tection : 99.38%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. R√©cup√©rer uniquement les lignes spam (Label = 1)\n",
    "spam_only = test_data[test_data['Label'] == 1].copy()\n",
    "print(f\"\\nüìå Nombre total de requ√™tes spam dans test_data : {len(spam_only)}\")\n",
    "\n",
    "# 2. Nettoyage si besoin\n",
    "spam_only['Sentence'] = spam_only['Sentence'].apply(clean_text)\n",
    "\n",
    "# 3. Tokenisation + Padding\n",
    "X_spam = tokenizer.texts_to_sequences(spam_only['Sentence'])\n",
    "X_spam = pad_sequences(X_spam, padding='post', maxlen=max_len)\n",
    "\n",
    "# 4. Pr√©diction\n",
    "spam_preds = model.predict(X_spam)\n",
    "spam_preds_labels = (spam_preds > 0.5).astype(int)\n",
    "\n",
    "# 5. Calcul du nombre de spams correctement d√©tect√©s\n",
    "true_positives = np.sum(spam_preds_labels == 1)\n",
    "total_spams = len(spam_only)\n",
    "detection_rate = (true_positives / total_spams) * 100\n",
    "\n",
    "# 6. Affichage\n",
    "print(f\"‚úÖ Spams correctement d√©tect√©s : {true_positives}\")\n",
    "print(f\"üìä Taux de d√©tection : {detection_rate:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8ea95-227c-4d69-b861-4f6d16d84ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
